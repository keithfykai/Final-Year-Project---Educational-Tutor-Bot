## Chapter S2A: Discrete Random Variables

**SYLLABUS INCLUDES**

\(\bullet\) Concepts of discrete random variables, probability distributions, expectations and variances

**PRE-REQUISITES**

\(\bullet\) Probability

\(\bullet\) Descriptive Statistics (See **Appendix**)

**CONTENT**

1. Random Variables

1.1 Definition

1.2 Discrete versus Continuous Random Variables

1.3 Probability Distribution of a Discrete Random Variable

1.4 Expectation of a Discrete Random Variable

1.5 Relationship between Expectation and Sample Mean

1.6 Independent Random Variables

1.7 Functions of a Discrete Random Variable

1.8 Variance and Standard Deviation of a Discrete Random Variable

1.9 Linear Combination of Independent Random Variables

1.10 Miscellaneous examples

Appendix:

A A Simple Application of Statistics in Insurance

B Descriptive Statistics (Assumed knowledge)

**INTRODUCTION**

It often happens in probability that the events we are interested in involve counting or measuring something.

When we toss two dice, what are the possible outcomes we can measure?

\(\bullet\) Number of '1's obtained

Number of even outcomes

Sum of two dice, etc.

For instance, when tossing a pair of dice, we are often interested in the sum of the two dice and not overly concerned over the value of each individual die. That is, we may be only interested in knowing that the sum is 7 and not whether the actual outcome is {1,6}, {2,5}, {3,4}, {4,3}, {5,2} or {6,1}.

Also in coin-flipping, we may be only interested in the number of heads obtained and not be too concerned about the actual head-tail sequences that result.

These quantities of interests, or more formally, these real value functions defined on the sample space, are known as **random variables**.

## 1 Random variable

### Definition

A **random variable** is a quantity that takes different numerical values according to the result of a random experiment whose outcome cannot be predicted exactly.

For example, consider the experiment of tossing a fair coin 3 times and observe the outcome. The sample space \(\Omega\) may be represented by

\[\{\text{HHH, HHT, HTH, THH, HTT, THT, TTT}\}.\]

Let's denote the number of heads obtained when tossing the fair coin 3 times by \(X\).

* The possible values \(X\) can take are 0, 1, 2 and 3 (**variable**).
* The value it assumes is subject to chance (**random**), so \(X\) is a random variable.
* In general, P (\(X=x\)) refers to the probability of the random variable \(X\) assuming a specific value \(x\).
* Eg. P(\(X=2\)) refers to the probability of obtaining 2 heads in the 3 tosses.

### Discrete versus Continuous Random Variables

A **discrete random variable** takes on a _countable_1 number of possible values while a **continuous random variable** can take any value in a given range.

Footnote 1: A countable set is one whose elements are in 1-1 correspondence with the positive integers.

Which of the following are discrete random variables and which are continuous?

* The number of phone calls a person picks up in a minute
* The life span of a light-bulb manufactured by a factory
* The number of students who are late for school on a particular day
* The distance travelled by a car in a week
* The height of a student from the Class of 2020In general, random variables that represent the number of occurrences of an event or objects are most suited for discrete random variables while those that measure spatial dimensions, time, temperature, weight etc, are best described by continuous random variables.

Very often, random variables are associated with their **probability distributions (**for discrete random variables)** or probability density functions (for continuous random variables), as well as **expectation** and **variance.** In this chapter, we will focus on discrete random variables and their associated probability distributions, expectation and variance.

### Probability Distribution of a Discrete Random Variable

For a random experiment,

Let \(S\) be the sample space of the experiment (e.g. outcomes of tossing 3 coins).

Let \(X\) be the random variable defined to take values of the experiments (e.g. number of heads). A table or formula giving the values of \(\mathrm{P}(X=x)\) for every \(x\) in \(S\) is called the **probability distribution** of \(X\).

For the experiment of tossing a fair coin 3 times where \(X\) is the number of heads obtained, the **probability distribution** of \(X\) is as follows

\begin{tabular}{|c|c|c|c|c|} \hline \(x\) & 0 & 1 & 2 & 3 \\ \hline \(\mathrm{P}(X=x)\) & \(\dfrac{1}{8}\) & \(\dfrac{3}{8}\) & \(\dfrac{3}{8}\) & \(\dfrac{1}{8}\) \\ \hline \end{tabular}

Observe that the probability distribution of \(X\) satisfy the following:

\begin{tabular}{|c|c|c|c|} \hline \(1\). & \(0\leq\mathrm{P}(X=x)\leq 1\) & for all \(x\) in \(S\). \\ \(2\). & \(\sum_{x=S}\mathrm{P}(X=x)=1\) & where the summation is over all values of \(x\) in \(S\). \\ \hline \end{tabular}

### Expectation of a Discrete Random Variable

The **expectation** (or **mean**, or **expected value**) of a discrete random variable \(X\) is the weighted average of the possible values that \(X\) can take on, each value being weighted by its corresponding probability.

It is denoted by **E(X)** or \(\boldsymbol{\mu}\) and is a measure of the 'centre' of the probability distribution of \(X\).

Formally, if \(X\) is a discrete random variable taking values from a set \(S\), then

\begin{tabular}{|c|} \hline \(\mathrm{E}(X)=\sum_{x\in S}x\,\mathrm{P}(X=x)\). \\ \hline \end{tabular}

**Example 1**: Let \(X\) be the random variable representing the sum of scores obtained when 2 fair dice are thrown. State the smallest set \(S\) that contains all the possible values of \(X\). Tabulate the probability distribution of \(X\). Find E(\(X\)).

**Solution:**

\(S=\{2\), 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}

**Example 2**: The probability distribution of a discrete random variable is given by

\[\mathrm{P}(Y=y)=\begin{cases}\frac{1}{10}\,,&\text{if}\qquad y=0,\\ \\ \mathcal{C}y^{2}\,,&\text{if}\,y=1,2,3,4.\end{cases}\]

Given that \(c\) is a constant, find the value of \(c\). With this value of \(c\), find \(\mathrm{E}(Y)\).

**Solution:**

As \[\sum_{y=0}^{4}\mathrm{P}(Y=y)=1\] \[\Rightarrow\frac{1}{10}+c+4c+9c+16c=1\] \[\Rightarrow 30c=\frac{9}{10}\] \[\Rightarrow c=\frac{9}{300}\] \[\mathrm{E}(Y)=\sum_{y=0}^{4}y\,\mathrm{P}(Y=y)\] \[=0\bigg{(}\frac{1}{10}\bigg{)}+\sum_{y=1}^{4}\bigg{(}\frac{3}{100 }y^{3}\bigg{)}\] \[=3\]

### Relationship between Expectation and Sample Mean

Consider the number of heads obtained when we toss 3 fair coins. Denoting the number of heads obtained by \(X\), the possible values \(X\) can take are 0, 1, 2 or 3. By working out the values of \(\mathrm{P}(X=0)\), \(\mathrm{P}(X=1)\), \(\mathrm{P}(X=2)\) and \(\mathrm{P}(X=3)\), the probability distribution of \(X\) is presented in the following table.

\begin{tabular}{|c|c|c|c|c|c|} \hline \(x\) & 0 & 1 & 2 & 3 \\ \hline \(\mathrm{P}(X=x)\) & \(\frac{1}{2}\times\frac{1}{2}\times\frac{1}{2}=\frac{1}{8}\) & \(\frac{1}{2}\times\frac{1}{2}\times\frac{1}{2}\times 3=\frac{3}{8}\) & \(\frac{1}{2}\times\frac{1}{2}\times\frac{1}{2}\times 3=\frac{3}{8}\) & \(\frac{1}{2}\times\frac{1}{2}\times\frac{1}{2}=\frac{1}{8}\) \\  & =0.125 & & =0.375 & =0.375 & =0.125 \\ \hline \end{tabular}

Based on the probability distribution,

\[\mathrm{E}(X)=\sum_{x=0}^{3}x\,\mathrm{P}(X=x)=0(0.125)+\mathrm{I}(0.375)+2(0.3 75)+3(0.125)=1.5\]However, if we toss the 3 coins 50 times and observe the number of heads obtained in each of the tosses, the data obtained can be represented in a table, known as a _frequency distribution_. From the experiment, we _may_ end up with a data set as shown, which also includes the relative frequency distribution.

\begin{tabular}{|l|c|c|c|c|} \hline \(X\) & 0 & 1 & 2 & 3 \\ \hline Frequency, \(f\) & 5 & 20 & 18 & 7 \\ \hline Relative frequency & \(\dfrac{5}{50}=0.1\) & \(\dfrac{20}{50}=0.4\) & \(\dfrac{18}{50}=0.36\) & \(\dfrac{7}{50}=0.14\) \\ \hline \end{tabular}

Based on the above sample, mean number of heads (or sample mean) obtained in each toss is

\begin{tabular}{|l|c|c|c|} \hline \(\dfrac{\sum fx}{\sum f}=\dfrac{0(5)+1(20)+2(18)+3(7)}{50}=0\bigg{(}\dfrac{5}{5 0}\bigg{)}+1\bigg{(}\dfrac{20}{50}\bigg{)}+2\bigg{(}\dfrac{18}{50}\bigg{)}+3 \bigg{(}\dfrac{7}{50}\bigg{)}=1.54\) \\ \hline \end{tabular}

If we increase the number of tosses to \(n\) times where \(n\) is large, we would expect the result for the relative frequency distribution to be close to the result for the probability distribution. (Refer to Appendix \(A\) in lecture notes of Chapter S1B: Probability, the empirical approach suggests that \(\dfrac{f}{\sum f}=\dfrac{f}{n}\) tends to the actual probability of the event as \(n\rightarrow\infty\) )

Then the expected value of \(X\) can be well approximated by the sample mean since the result for the relative frequency distribution would be close to the result for the probability distribution. We can also observe that the definition of the expected value (or mean) for a discrete random variable using a probability distribution is analogous to how we compute the sample mean for a set of data represented by a frequency distribution.

From the above, we understand intuitively why the _expected value_ of a discrete random variable is the mean i.e. mean of \(X\) would be given by \(\operatorname{E}(X)=\sum x\operatorname{P}(X=x)\).

### Independent Random Variables

We have discussed what it means for two events to be independent. The concept of independent random variables is actually more complicated and requires the notion of joint probability distributions. However, there is a need for you to have at least an intuitive idea of the concept, as it will help simplify calculations.

Let \(X\) and \(Y\) be two discrete random variables taking on possible values \(x_{1},x_{2},...\) and \(y_{1},y_{2},...\) respectively. The random variables \(X\) and \(Y\) are said to be **independent** if for all \(i\) and \(j\),

\[\operatorname{P}(X=x_{i}\text{ and }Y=y_{j})=\operatorname{P}(X=x_{i}) \operatorname{P}(Y=y_{j})\text{.}\]

You should see the similarity of this definition with that of independence of events introduced earlier.

**Example 3**:

A computer generates a random variable \(X\) whose probability distribution is given in the following table:

\begin{tabular}{|c|c|c|c|} \hline \(x\) & 0 & 2 & 4 \\ \hline P(\(X=x\)) & 0.4 & 0.3 & 0.3 \\ \hline \end{tabular} Find E(\(X\)).

Two independent observations of \(X\) are denoted by \(X_{1}\) and \(X_{2}\). Let \(Y=X_{1}+X_{2}\). Show that P(\(Y=6\)) = 0.18.

Tabulate the probability distribution of \(Y\) and find E(\(Y\)). Comment on the relationship between the values of E(\(Y\)) and E(\(X\)).

**Solution:**

E(\(X\)) = \(\sum x\) P(\(X=x\)) = 0 x 0.4 + 2 x 0.3 + 4 x 0.3 = 1.8

\(Y=X_{1}+X_{2}\)

P(\(Y=6\)) = P(\(X_{1}+X_{2}=6\))

= P(\(X_{1}=2\) & \(X_{2}=4\), \(X_{1}=4\) & \(X_{2}=2\))

= 2 (0.3 x 0.3)

= 0.18 (shown)

\(Y\) can take the values {0, 2, 4, 6, 8}.

P(\(Y=0\)) = P(\(X_{1}=0\) & \(X_{2}=0\)) = 0.4 x 0.4 = 0.16

P(\(Y=2\)) = P(\(X_{1}=2\) & \(X_{2}=0\), \(X_{1}=0\) & \(X_{2}=2\))

= \(\big{(}0.3\times 0.4\big{)}+\big{(}0.4\times 0.3\big{)}=0.24\)

P(\(Y=4\)) = P(\(X_{1}=4\) & \(X_{2}=0\), \(X_{1}=2\) & \(X_{2}=2\), \(X_{1}=0\) & \(X_{2}=4\))

= 2 (0.3 x 0.4) + \(\big{(}0.3\times 0.3\big{)}=0.33\)

P(\(Y=8\)) = P(\(X_{1}=4\) & \(X_{2}=4\)) = 0.3 x 0.3 = 0.09

\(\begin{array}{|c|c|c|c|c|}\hline y&0&2&4&6&8\\ \hline P(Y=y)&0.16&0.24&0.33&0.18&0.09\\ \hline\end{array}\)

E(\(Y\)) = \(\sum y\) P(\(Y=y\)) = 0 + 0.48 + 1.32 + 1.08 + 0.72 = 3.6

It is noted that E(\(Y\)) = E(\(X_{1}+X_{2}\)) = 2E(\(X\)) Is this a general result?

### Functions of a Discrete Random Variable

A function of a random variable is itself a random variable. Thus if \(X\) is a random variable, then functions such as \(\cos X\), \(X^{2}\), \(2X+4\), \(\left(X-\mathrm{E}\left(X\right)\right)^{2}\) are also random variables. If, in addition, \(X\) is a _discrete_ random variable, then every function of \(X\) is a _discrete_ random variable.

The expectation of \(\mathrm{g}(X)\)**,** where \(\mathrm{g}\) is a function of \(X\), can be obtained as follows:

\[\boxed{\mathrm{E}(\mathrm{g}(X))=\sum_{x\in S}\mathrm{g}(x)\,\mathrm{P}(X=x).}\]

In particular,

\[\boxed{\mathrm{E}(X^{2})=\sum_{x\in S}x^{2}\,\mathrm{P}(X=x).}\]

From example 3, the probability distribution of the random variable \(X\) is given in the following table:

\[\boxed{\begin{array}{|c|c|c|c|}\hline x&0&2&4\\ \hline\mathrm{P}(X=x)&0.4&0.3&0.3\\ \hline\end{array}}\]

To calculate \(\mathrm{E}(X^{2})\), we can add a row showing the values of \(X^{2}\).

\[\boxed{\begin{array}{|c|c|c|}\hline x&0&2&4\\ \hline x^{2}&0&4&16\\ \hline\mathrm{P}(X=x)&0.4&0.3&0.3\\ \hline\end{array}}\]

\[\mathrm{E}(X^{2})=\sum x^{2}\,\mathrm{P}(X=x)=0\times 0.4+4\times 0.3+16\times 0.3=6\]

To calculate \(\mathrm{E}(2X+1)\), we can add a row showing the values of \(2X+1\).

\[\boxed{\begin{array}{|c|c|c|c|}\hline x&0&2&4\\ \hline 2x+1&1&5&9\\ \hline\mathrm{P}(X=x)&0.4&0.3&0.3\\ \hline\end{array}}\]

\[\mathrm{E}(2X+1)=1\times 0.4+5\times 0.3+9\times 0.3=4.6\]

In general we have the following properties:

Let \(X\) and \(Y\) be random variables and \(a\) and \(b\) be constants.

We have

\[\begin{array}{|c|c|}\hline\textbf{(1)}&\mathrm{E}(a)=a\\ \textbf{(2)}&\mathrm{E}(aX)=a\,\mathrm{E}(X)\\ \textbf{(3)}&\mathrm{E}(aX\pm b)=a\mathrm{E}(X)\pm b\\ \end{array}\]

If \(X\) and \(Y\) are random variables, then

\[\textbf{(4)}&\mathrm{E}(aX\pm bY)=a\mathrm{E}(X)\pm b\mathrm{E}(Y)\\ \hline\end{array}\]Proof for **(1)**: \(\mathrm{E}(a)=\sum a\,\mathrm{P}(X=x)\)

\[=a\,\sum\mathrm{P}(X=x)\] \[=a\,\big{(}1\big{)}\] \[=a\,\]

Proof for **(3)**: \(\mathrm{E}(aX+b)=\sum\big{(}\alpha x+b\big{)}\,\mathrm{P}(X=x)\)

\[= a\sum x\,\mathrm{P}(X=x)+b\sum\,\mathrm{P}(X=x)\] \[= a\,\mathrm{E}\big{(}X\big{)}+b\]

### Variance and Standard Deviation of a Discrete Random Variable

The **variance** of a discrete random variable \(X\) is a measure of the _expected spread of a random variable about its mean_. It is defined as the expectation of \(\big{(}X-\mu\big{)}^{2}\), where \(\mu=\mathrm{E}\big{(}X\big{)}\) and is denoted by **Var(\(X\))** or \(\sigma^{2}\).

In the case of a discrete random variable, the variance is given by

\[\boxed{\mathrm{Var}(X)=\mathrm{E}(X-\mu)^{2}}\] \[= \sum\big{(}x-\mu\big{)}^{2}\,\mathrm{P}\big{(}X=x\big{)}\]

The **standard deviation** of \(X\), denoted by \(\sigma\), is defined as \(\sqrt{\mathrm{Var}(X)}\).

Although the variance of \(X\), by definition, is given by \(\mathrm{E}\Big{[}\big{(}X-\mu\big{)}^{2}\Big{]}\), for computational purposes, we often use the following equivalent equation,

\[\boxed{\mathrm{Var}\big{(}X\big{)}=\mathrm{E}\big{(}X^{2}\big{)}-\mu^{2}}\] \[= \mathrm{E}\big{(}X^{2}\big{)}-\big{[}\mathrm{E}(X)\big{]}^{2}\]

Proof: \[\mathrm{E}\Big{[}\big{(}X-\mu\big{)}^{2}\Big{]}= \mathrm{E}\big{(}X^{2}-2\mu X+\mu^{2}\big{)}\] \[= \mathrm{E}(X^{2})-2\mu\,\mathrm{E}(X)+\mu^{2}\] \[= \mathrm{E}(X^{2})-2\mu^{2}+\mu^{2}\] \[= \mathrm{E}(X^{2})-\mu^{2}\]In a similar way, one would be able to see that the definition of the variance for a discrete random variable using a probability distribution is analogous to how we compute the variance for data represented by a frequency distribution.

For a discrete random variable, \(\operatorname{Var}(X)=\operatorname{E}(X^{2})-\mu^{2}=\sum x^{2}\ \operatorname{P}(X=x)-\mu^{2}\)

while variance is \(\dfrac{1}{\sum f}\Bigl{(}\sum f\!\!x^{2}\Bigr{)}-\bigl{(}\,\overline{x}\, \bigr{)}^{2}\) for a set of observed data.

Consider the probability distributions of the random variables \(X\) and \(Y\).

\begin{tabular}{|c|c|c|c|c|} \hline \(x\) & 1 & 2 & 3 & 4 & 5 \\ \hline \(\operatorname{P}(X=x)\) & \(\dfrac{1}{5}\) & \(\dfrac{1}{5}\) & \(\dfrac{1}{5}\) & \(\dfrac{1}{5}\) & \(\dfrac{1}{5}\) \\ \hline \(y\) & 1 & 2 & 3 & 4 & 5 \\ \hline \(\operatorname{P}(Y=y)\) & \(\dfrac{1}{50}\) & \(\dfrac{4}{50}=\dfrac{2}{25}\) & \(\dfrac{40}{50}=\dfrac{4}{5}\) & \(\dfrac{4}{50}=\dfrac{2}{25}\) & \(\dfrac{1}{50}\) \\ \hline \end{tabular}

By symmetry, \(\operatorname{E}(X)=\operatorname{E}(Y)=3\)

\begin{tabular}{|l|l|l|} \hline \(\operatorname{Var}\bigl{(}X\bigr{)}\) & \(\operatorname{Var}\bigl{(}Y\bigr{)}\) \\ \(=\operatorname{E}\bigl{(}X^{2}\bigr{)}-\bigl{[}\operatorname{E}\bigl{(}X \bigr{)}\bigr{]}^{2}\) & \(=\operatorname{E}\bigl{(}Y^{2}\bigr{)}-\bigl{[}\operatorname{E}\bigl{(}Y \bigr{)}\bigr{]}^{2}\) \\ \(=\biggl{(}1\times\dfrac{1}{5}+4\times\dfrac{1}{5}+9\times\dfrac{1}{5}+16\times \dfrac{1}{5}+25\times\dfrac{1}{5}\biggr{)}-3^{2}\) & \(=\biggl{(}1\times\dfrac{1}{50}+4\times\dfrac{4}{50}+9\times\dfrac{40}{50}+16 \times\dfrac{4}{50}+25\times\dfrac{1}{50}\biggr{)}-3^{2}\) \\ \(=11-9=2\) & \(=9.32-9=0.32\) \\ \hline \end{tabular}

**Note:**

Even though \(X\) and \(Y\) have the same expectations, their variances are different. \(\operatorname{Var}(X)\) is large as the observations of \(X\) are widely scattered from the mean. \(\operatorname{Var}(Y)\) is small as observations of \(Y\) tend to cluster close to the mean.

We can use the GC to help check our answers:

\begin{tabular}{|l|l|l|} \hline \(1\). & Press **stat** and select **1:Edit...** & \(\operatorname{\underline{\textbf{1}}}\) \\  & Enter the data \(\operatorname{into}\ \mathbf{L_{1}},\mathbf{L_{2}},\mathbf{L_{3}}\). & \(\operatorname{\underline{\textbf{1}}}\) \\ \hline \end{tabular}

Chapter S2A: Discrete Random Variables

Page 10 of 21

\begin{tabular}{|l l|} \hline
2. & Press **[stat]** and scroll to the right to select. \\  & **CALC.** Select **1: 1-Var Stats. \\  & & 2:2-Var Stats \\  & & 3:Med-Med \\  & & 4:LinReg(ax+b) \\  & & 5:QuadeReg \\  & & 6:CubicReg \\  & & 7:QuadeReg \\  & & 8:LinReg(ax+bx) \\  & & 94LnReg \\ \hline
3. & Enter **L1** under **List.** & \\  & Enter **L2** (for _X_) under **FreqList.** & \\  & Scroll down to select **Calculate**. & \\ \hline
4. & Press **[enter]** to obtain the following screen. & \\  & & \(\text{E}\left(X\right)=\overline{x}=3\), \(\text{E}\left(X^{2}\right)=11\) \\  & & \(\text{Var}\left(X\right)=\text{E}\left(X^{2}\right)-\left[\text{E}\left(X \right)\right]^{2}\) \\  & & \(=11-3^{2}=2=\left(\sigma x\right)^{2}\) \\ \hline
5. & Enter **L3** (for _Y_) under **FreqList.** & \\  & Scroll down to select **Calculate**. & \\ \hline
6. & Press **[enter]** to obtain the following screen. & \\  & & \(\text{E}\left(Y\right)=\overline{y}=3\), \(\text{E}\left(Y^{2}\right)=9.32\) \\  & & \(\text{Var}\left(Y\right)=\text{E}\left(Y^{2}\right)-\left[\text{E}\left(Y \right)\right]^{2}\) \\  & & \(=9.32-3^{2}\) \\  & & \(=0.32=\left(\sigma y\right)^{2}\) \\ \hline \end{tabular}

Chapter S2A: Discrete Random VariablesWe have the following properties:

\begin{tabular}{|l l|l|l|} \hline \multicolumn{1}{|c|}{Let \(X\)} & \multicolumn{1}{c|}{and \(Y\)} & \multicolumn{1}{c|}{be random variables and \(a\) and \(b\)} & \multicolumn{1}{c|}{be constants.} \\ \multicolumn{1}{|c|}{We have} \\ \multicolumn{1}{|c|}{**(1)**} & \(\operatorname{Var}(a)=0\) \\ \multicolumn{1}{|c|}{**(2)**} & \(\operatorname{Var}(aX)=a^{2}\operatorname{Var}(X)\) \\ \multicolumn{1}{|c|}{**(3)**} & \(\operatorname{Var}(aX\pm b)=a^{2}\operatorname{Var}(X)\) \\ \end{tabular}

If \(X\) and \(Y\) are _independent_ random variables, then

\(\operatorname{Var}(aX\pm bY)=a^{2}\operatorname{Var}(X)+b^{2}\operatorname{Var }(Y)\)

**Note:** The variance of a constant is zero. This is to be expected as a constant has no variation!

### Linear Combination of Independent Random Variables

Given a random variable \(X\), the probability distribution of \(2X\) and \(X_{1}+X_{2}\), where \(X_{1}\) and \(X_{2}\) are two independent observations of \(X\), are totally different. This is illustrated using the example below, and we will return to this important concept again in Chapter 3.

#### Example 4

When a tetrahedral die is thrown, the number on the face on which it lands, \(X\), has the following probability distribution

\begin{tabular}{|c|c|c|c|c|} \hline \(x\) & 1 & 2 & 3 & 4 \\ \hline \(\operatorname{P}(X=x\)) & 0.25 & 0.25 & 0.25 & 0.25 \\ \hline \end{tabular}

It is given that \(\operatorname{E}(X)=2.5\) and \(\operatorname{Var}(X)=1.25\).

* Tabulate the probability distribution of \(D\), the random variable representing 'double the number on which the die lands'. Find \(\operatorname{E}(D)\) and \(\operatorname{Var}(D)\).
* Tabulate the probability distribution function of \(S\), the random variable representing 'the sum of the two numbers when the die is thrown twice independently'. Find \(\operatorname{E}(S)\) and \(\operatorname{Var}(S)\).

Comment on the relationship between the values of

* \(\operatorname{E}(D)\) and \(\operatorname{E}(S)\); (b) \(\operatorname{Var}(D)\) and \(\operatorname{Var}(S)\).

\begin{tabular}{|c|c|c|c|c|} \hline \multicolumn{5}{|c|}{**Solution:**} \\ \multicolumn{5}{|c|}{(i) \(D=2X\)} \\ \hline \(d\) & \(2\) & \(4\) & \(6\) & \(8\) \\ \hline \(\mathrm{P}(D=d\) ) & \(\dfrac{1}{4}\) & \(\dfrac{1}{4}\) & \(\dfrac{1}{4}\) & \(\dfrac{1}{4}\) \\ \hline \(\mathrm{E}(D)=\dfrac{1}{4}\big{(}2+4+6+8\big{)}=5\) [or by symmetry] \\ \(\mathrm{E}(D^{2})=\dfrac{1}{4}\big{(}4+16+36+64\big{)}=30\) \\ \(\mathrm{Var}(D)=\mathrm{E}(D^{2})-\big{[}\mathrm{E}(D)\big{]}^{2}=30-5^{2}=5\) \\ (ii) & \(S=X_{1}+X_{2}\) \\ \hline \(\mathbf{1}\) & \(2\) & \(3\) & \(4\) \\ \hline \(\mathbf{2}\) & \(3\) & \(4\) & \(5\) \\ \hline \(\mathbf{3}\) & \(4\) & \(5\) & \(6\) & \(7\) \\ \hline \(\mathbf{4}\) & \(5\) & \(6\) & \(7\) & \(8\) \\ \hline \end{tabular}

Probability distribution of \(S\)

\begin{tabular}{|c|c|c|c|c|c|c|} \hline \(s\) & \(2\) & \(3\) & \(4\) & \(5\) & \(6\) & \(7\) & \(8\) \\ \hline \(\mathrm{P}(S=s\) ) & \(\dfrac{1}{16}\) & \(\dfrac{2}{16}=\dfrac{1}{8}\) & \(\dfrac{3}{16}\) & \(\dfrac{4}{16}=\dfrac{1}{4}\) & \(\dfrac{3}{16}\) & \(\dfrac{2}{16}=\dfrac{1}{8}\) & \(\dfrac{1}{16}\) \\ \hline \end{tabular}

\(\mathrm{E}(S)=\dfrac{1}{16}\big{(}2+3(2)+4(3)+5(4)+6(3)+7(2)+8\big{)}=5\) [or by symmetry] \\ \(\mathrm{E}(S^{2})=\dfrac{1}{16}\big{(}2^{2}+3^{2}(2)+4^{2}(3)+5^{2}(4)+6^{2} (3)+7^{2}(2)+8^{2}\big{)}=27.5\) \\ \(\mathrm{Var}(S)=\mathrm{E}(S^{2})-\big{[}\mathrm{E}(S)\big{]}^{2}=27.5-5^{2}=2.5\) \\ (a) The means of the two distributions, \(D\) and \(S\) are the same, i.e. \(\mathrm{E}(D)=\mathrm{E}(S)\).

(b) The variances are **not** the same, with the random variable \(D\) having the greater variance.

**Important Results:**

Notice that \[\begin{array}{ll}\text{E}(D)=\text{E}(2\,X)=2\,\text{E}(X)\\ \text{E}(S)=\text{E}(X_{1}+X_{2})=\text{E}(X_{1})+\text{E}(X_{2})=2\,\text{E}(X) \end{array}\] but \[\begin{array}{ll}\text{Var}(D)=\text{Var}(2\,X)=2^{2}\,\text{Var}(X)=4 \,\text{Var}(X)\\ \text{Var}(S)=\text{Var}(X_{1}+X_{2})=\text{Var}(X_{1})+\text{Var}(X_{2})=2\, \text{Var}(X)\end{array}\]

**Question:**

Is \(2X\) the same as \(X_{1}+X_{2}\)? Why?

\(2X\) is twice the observation of \(X\) but \(X_{1}+X_{2}\) is the sum of two observations of \(X\).

Examples:

* Two identical jars, Jar 1 and Jar 2, each with 5 black marbles and 4 white marbles in it. \(X_{k}\) : the number of black marbles drawn from 3 draws, without replacement, from Jar \(k\)\(X_{1}+X_{2}\) : the total number of black marbles drawn from Jars 1 and 2. \(X\) : the number of black marbles drawn from 3 draws, without replacement, from a jar. \(2X\) : twice the number of black marbles drawn from a jar.
* \(2X\) denotes 'twice the number of students present in school on a particular day' while \(X_{1}\)\(+X_{2}\) denotes the sum of the number of students present on two different days.

In general, \[\begin{array}{ll}\text{E}(nX)=n\,\text{E}(X)\\ \text{E}(X_{1}+X_{2}+...+X_{n})=\text{E}\big{(}X_{1}\big{)}+\text{E}\big{(}X_{2 }\big{)}+...+\text{E}\big{(}X_{n}\big{)}=n\,\text{E}\big{(}X\big{)}\end{array}\] but \[\begin{array}{ll}\text{Var}(nX)=n^{2}\,\text{Var}(X)\\ \text{Var}(X_{1}+X_{2}+...+X_{n})=\text{Var}(X_{1})+\text{Var}(X_{2})+...+\text {Var}\big{(}X_{n}\big{)}=n\,\text{Var}(X)\end{array}\]

### Miscellaneous Examples

#### Example 5 MJCPrelim/9758/2018/02/Q6

A bag contains five balls numbered 1 to 5. A game consists of a player taking two balls from the bag consecutively, with replacement. The number on each ball is noted down. His score \(X\) is found by taking the absolute value of the difference between the two numbers.

**(i)**: Obtain the probability distribution of \(X\).
**(ii)**: Find \(\operatorname{E}\left(X\right)\) and \(\operatorname{Var}\left(X\right).\)

A player pays \(\$\)\(a\) to play this game. He wins an amount (in dollars) corresponding to twice his score. Determine the range of values of \(a\) if the player is expected to make a loss.

**Solution:**

(i) Let \(X\) denote the absolute value of the difference between the two numbers.

\begin{tabular}{|l|l|l|l|l|l|} \hline \(X\) & **1** & **2** & **3** & **4** & **5** \\ \hline
**1** & 0 & 1 & 2 & 3 & 4 \\ \hline
**2** & 1 & 0 & 1 & 2 & 3 \\ \hline
**3** & 2 & 1 & 0 & 1 & 2 \\ \hline
**4** & 3 & 2 & 1 & 0 & 1 \\ \hline
**5** & 4 & 3 & 2 & 1 & 0 \\ \hline \end{tabular}

The probability distribution of \(X\) is as follows:

\begin{tabular}{|l|l|l|l|l|l|} \hline \(X\) & 0 & 1 & 2 & 3 & 4 \\ \hline \(\operatorname{P}\left(X=x\right)\) & \(\dfrac{5}{25}=\dfrac{1}{5}\) & \(\dfrac{8}{25}\) & \(\dfrac{6}{25}\) & \(\dfrac{4}{25}\) & \(\dfrac{2}{25}\) \\ \hline \end{tabular}

(ii) Using GC, \(\operatorname{E}\left(X\right)=1.6,\operatorname{Var}\left(X\right)=1.44\)

Amount that he wins \(=2X\)

If the player is expected to make a loss,

\(\operatorname{E}(2X-a)<0\)

\(2\left(1.6\right)-a<0\)

\(\therefore a>3.2\)

**Example 6**: **TPJCPrelim/9758/2018/02/Q5**

An unbiased die has three faces painted red, two faces painted green and one face painted blue. A red face has a score of 1 point, a green face has a score of 2 points and a blue face has a score of 3 points.

Two such dice are thrown and the sum of their scores is denoted by \(X\).

**(i)** Show that \(\mathrm{P}(X=4)=\dfrac{5}{18}\) and find the probability distribution of \(X\).
**(ii)** Find \(\mathrm{E}(X)\) and show that \(\mathrm{Var}\left(X\right)=\dfrac{10}{9}\).

Suppose now a red face has a score of 3 points, a green face has a score of 2 points and a blue face has a score of 1 point.
**(iii)** Deduce the expectation and variance of the sum of scores obtained from a throw of two such dice.

**Solution:**

(i)

\(\mathrm{P}\left(X=2\right)=\mathrm{P}(\mathrm{red})\times\mathrm{P}(\mathrm{ red})=\dfrac{3}{6}\dfrac{3}{6}=\dfrac{1}{2}\times\dfrac{1}{2}=\dfrac{1}{4}\)

\(\mathrm{P}\left(X=3\right)=\mathrm{P}(\mathrm{red})\times\mathrm{P}\big{(} \mathrm{green}\big{)}+\mathrm{P}\big{(}\mathrm{green}\big{)}\times\mathrm{P} (\mathrm{red})\)

\(\mathrm{=\dfrac{3}{6}\times\dfrac{2}{6}+\dfrac{2}{6}\times\dfrac{3}{6}=\dfrac {1}{2}\times\dfrac{1}{3}\times 2=\dfrac{1}{3}\)

\(\mathrm{P}\left(X=4\right)=\mathrm{P}(\mathrm{red})\times\mathrm{P}\big{(} \mathrm{blue}\big{)}+\mathrm{P}\big{(}\mathrm{blue}\big{)}\times\mathrm{P} (\mathrm{red})+\mathrm{P}\big{(}\mathrm{green}\big{)}\times\mathrm{P}\big{(} \mathrm{green}\big{)}\)

\(\mathrm{=\dfrac{3}{6}\times\dfrac{1}{6}+\dfrac{1}{6}\times\dfrac{3}{6}+\dfrac{2 }{6}\times\dfrac{2}{6}=\dfrac{1}{2}\times\dfrac{1}{6}\times 2+\dfrac{1}{3}\times \dfrac{1}{3}=\dfrac{5}{18}\)

\(\mathrm{P}\left(X=5\right)=\mathrm{P}\big{(}\mathrm{green}\big{)}\times\mathrm{P }\big{(}\mathrm{blue}\big{)}+\mathrm{P}\big{(}\mathrm{blue}\big{)}\times \mathrm{P}\big{(}\mathrm{green}\big{)}\)

\(\mathrm{=\dfrac{2}{6}\times\dfrac{1}{6}+\dfrac{1}{6}\times\dfrac{2}{6}=\dfrac {1}{3}\times\dfrac{1}{6}\times 2=\dfrac{1}{9}\)

\(\mathrm{P}\left(X=6\right)=\mathrm{P}\big{(}\mathrm{blue}\big{)}\times\mathrm{P }\big{(}\mathrm{blue}\big{)}\)

\(\mathrm{=\dfrac{1}{6}\times\dfrac{1}{6}=\dfrac{1}{36}\)

Probability distribution of \(X\) :

\(\begin{array}{|c|c|c|c|c|}\hline x&2&3&4&5&6\\ \hline\mathrm{P}\left(X=x\right)&\dfrac{1}{4}&\dfrac{1}{3}&\dfrac{5}{18}& \dfrac{1}{9}&\dfrac{1}{36}\\ \hline\end{array}\)

Chapter S2A: Discrete Random Variables

Page 16 of 21(ii) \[\begin{split}\operatorname{E}\left(X\right)&=\sum_{x=2}^{6 }x\cdot\operatorname{P}\left(X=x\right)\\ &=2\times\frac{1}{4}+3\times\frac{1}{3}+4\times\frac{5}{18}+5 \times\frac{1}{9}+6\times\frac{1}{36}\\ &=\frac{10}{3}\\ \operatorname{E}\left(X^{2}\right)&=\sum_{x=2}^{6}x^{2} \cdot\operatorname{P}\left(X=x\right)\\ &=2^{2}\times\frac{1}{4}+3^{2}\times\frac{1}{3}+4^{2}\times \frac{5}{18}+5^{2}\times\frac{1}{9}+6^{2}\times\frac{1}{36}\\ &=\frac{110}{9}\\ \operatorname{Var}\left(X\right)&=\operatorname{E}\left( X^{2}\right)-\left[\operatorname{E}\left(X\right)\right]^{2}\\ &=\frac{110}{9}-\left(\frac{10}{3}\right)^{2}\\ &=\frac{10}{9}\\ \end{split}\]

(iii) Let \(Y\) be the sum of scores obtained from a throw of two dice where a red face has a score of 3 points, a green face has a score of 2 points and a blue face has a score of 1 point. Then \(Y=8-X\).

\[\operatorname{E}\left(Y\right)=\operatorname{E}\left(8-X\right)=8- \operatorname{E}\left(X\right)=8-\frac{10}{3}=\frac{14}{3}\\ \operatorname{Var}\left(Y\right)=\operatorname{Var}\left(8-X \right)=\operatorname{Var}\left(8\right)+\operatorname{Var}\left(X\right)= \operatorname{Var}\left(X\right)=\frac{10}{9}\\ \end{split}\]

## Appendix A:

### A Simple Application of Statistics in Insurance

Let's say on average a house is worth $600 000. A house owner could lose the house as the result of the action of fire, flood or accident. The chance of this happening is small, say \(\dfrac{1}{500}\) in any one year.

An insurance company which decided to undertake the risk charges a premium to cover the losses if any. If the company set this premium at $1,300 per annum and took over the risk of loss on 100,000 houses, then it would have an income of $130 million. On average, 200 of those 100,000 houses insured would burn down, and the company would pay out $120 million in compensation. The company is left with $10 million for operating expenses and profit, while the individual house owners are freed from uncertainty.

The important feature from the point of view of the insurance company is the long-run average of the sums of money they must pay out. This is called the expectation of loss. In this illustration, it can be seen that the average loss is $1,200 per house issued.

\[\begin{array}{lcl}\text{Expectation of loss}&=&\text{Size of loss }\times \text{Probability of that loss}\\ \text{E (loss)}&=&\$600\ 000\times\dfrac{1}{500}\\ &=&\$1200\end{array}\]

Now, the house owner has exchanged the possibility of losing $600 000 for the certainty of losing $1,300. From his point of view,

\[\begin{array}{lcl}\text{Expectation of loss}&=&\text{Size of loss }\times \text{Probability of that loss}\\ \text{E (loss)}&=&\$1300\times 1\\ &=&\$1300\end{array}\]

Before insuring against the possible loss, the expected loss was $1,200, so the house owner has paid a little extra to remove the uncertainty. In a well-developed market, competition helps to keep the premiums down.

Let's review some concepts on Descriptive Statistics.

## Appendix B:

**1 MEASURES OF CENTRAL TENDENCY (AVERAGE)**

We will discuss the 3 most popular ones, the mode, median and mean.

**1.1**: The **Mode** of a set of data is the value which occurs most frequently in the set of data.

**1.2**: The **Median** (also known as the 50th percentile) of a set of observations arranged in order of magnitude is

(i) the middle value if the number of observations is odd; and

(ii) the average of the 2 middle values if the number of observations is even.

It is the value below which half of the distribution falls.

E.g. (i) The median of 7, 7, 2, 3, 4, 2, 7, 9 and 31 is 7.

(ii) The median of 36, 41, 27, 32, 29, 38, 39 and 43 is 37.

**Note:**

Other types of percentiles that are commonly computed are

1. The lower quartile \(\underline{Q}_{1}\) (also known as the 25th percentile), which is the value below which \(\dfrac{1}{4}\) of the observations lie.
2. The upper quartile \(\underline{Q}_{3}\) (also known as the 75th percentile), which is the value below which \(\dfrac{3}{4}\) of the observations lie.

**1.3**: **Mean**

(i) Ungrouped Data

Given a set of \(n\) observations \(x_{1},x_{2},...,x_{n}\), the mean is

\(\overline{x}=\dfrac{x_{1}+x_{2}+...+x_{n}}{n}=\dfrac{1}{n}\sum x_{i}\).

(ii) Grouped Data

If the observations \(x_{1},x_{2},...,x_{n}\) occur \(f_{1},f_{2},...,f_{n}\) times respectively,

the mean \(\overline{x}\) is

\(\overline{x}=\dfrac{\sum f_{i}x_{i}}{\sum f_{i}}\).

**Note:**

1. There is no "correct" average as each has its uses.
2. In statistics, the most important average is the mean because of its sensitivity to small changes in data and ease of computation.
3. The mean is strongly affected by extreme values and in such cases the median is a more suitable average.

## 2 MEASURES OF DISERSION / SPREAD

Consider the 3 sets of numbers:

(a) 7, 7, 7, 7, 7

(b) 4, 6, 6.5, 7.2, 11.3

(c) -193, -46, 28, 69, 177

They all have a mean of 7 but the spread of each data set is different.

There is no variability in (a), while the numbers in (c) are obviously much more spread out than those in (b).

There are various ways of measuring the dispersion / spread of data.

### Range = Largest observation - smallest observation.

### Interquartile range = \(Q_{3}-Q_{1}\)

#### Variance and Standard Deviation

(i) Ungrouped Data

Given a set of \(n\) observations \(x_{1},x_{2},...,x_{n}\), the variance is

\[\frac{1}{n}\sum\left(x_{i}-\overline{x}\right)^{2}=\frac{1}{n}\sum x_{i}^{2}- \left(\overline{x}\right)^{2}\.\]

(ii) Grouped Data

If the observations \(x_{1},x_{2},...,x_{n}\) occur \(f_{1},f_{2},...,f_{n}\) times respectively,

the variance is

\[\frac{1}{n}\sum f_{i}\left(x_{i}-\overline{x}\right)^{2}=\frac{1}{n}\sum f_{ i}x_{i}^{2}-\left(\overline{x}\right)^{2}\.\]

The standard deviation is given by \(\sqrt{\text{variance}}\).

#### Note:

Standard deviations are useful when comparing sets of data. In general,

\(\bullet\) a lower standard deviation indicates that the observations tend to cluster close to the mean (smaller variability/dispersion/spread), and

\(\bullet\) a higher standard deviation suggests that the observations are widely scattered from the mean (greater variability/dispersion/spread).

**Example 1**: Two machines A and B are used to pack biscuits. A random sample of ten packets was taken from each machine and the mass of each packet (to the nearest gram) was measured and noted. Find the mean and standard deviation of the masses for each of the two samples. Comment on the reliability of the two machines.

\begin{tabular}{|l|l|} \hline Machine A (\(x_{{}_{A}}\)) & 196, 198, 198, 199, 200, 200, 201, 201, 202, 205 \\ \hline Machine B (\(x_{{}_{B}}\)) & 192, 194, 195, 198, 200, 201, 203, 204, 206, 207 \\ \hline \end{tabular}

**Solution:**

For machine A,

sample mean mass of a packet, \(\overline{x}_{{}_{d}}=\dfrac{1}{10}\sum x_{{}_{d}}=\dfrac{2000}{10}=200\) g

Variance \(=\dfrac{1}{n}\sum x_{{}_{d}}^{2}-(\overline{x}_{{}_{d}})^{2}\)

\(=\dfrac{1}{10}(400056)-(200)^{2}\)

\(=5.6\) g\({}^{2}\)

Standard deviation \(=\sqrt{5.6}=2.37\)g (3 s.f.)

For machine B,

sample mean mass of a packet,

\(\overline{x}_{{}_{B}}=\dfrac{1}{10}\sum x_{{}_{B}}=\dfrac{2000}{10}=200\) g

Variance \(=\dfrac{1}{n}\sum x_{{}_{B}}^{2}-(\overline{x}_{{}_{B}})^{2}\)

\(=\dfrac{1}{10}(400240)-(200)^{2}\)

\(=24\) g\({}^{2}\)

Standard deviation \(=\sqrt{24}=4.90\) g (3 s.f.)

Machine A has less variability, indicating that it is more reliable than machine B.

**Example 2**: The frequency distribution of ages of a group of salesman under 30 is given as follows:

\begin{tabular}{|l|l|l|l|l|l|l|l|l|} \hline Age, \(x\) & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 \\ \hline Frequency, \(f\) & 3 & 3 & 4 & 6 & 7 & 4 & 2 & 1 \\ \hline \end{tabular}

Calculate the mean and standard deviation.

**Solution:**

Mean age, \(\overline{x}\), is \(\dfrac{\sum f\kappa}{\sum f}\)=\(\dfrac{726}{30}\)=24.2 years

Variance, Var(\(X\)) is \(\dfrac{1}{\sum f}\Big{(}\sum f\kappa^{2}\Big{)}-\big{(}\overline{x}\big{)}^{2 }=\dfrac{1}{30}(17668)-24.2^{2}\approx 3.2933\)

\(\therefore\) Standard deviation \(=\sqrt{\text{Var}(X)}=\)1.81 (3sf)

Chapter S2A: Discrete Random Variables

Page 21 of 21